{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics is one of the main building block of data science.\n",
      "STATISTICS IS ONE OF THE MAIN BUILDING BLOCK OF DATA SCIENCE.\n"
     ]
    }
   ],
   "source": [
    "input_str = 'Statistics is one of the main Building Block of Data Science.'\n",
    "lower = input_str.lower()\n",
    "print(lower)\n",
    "\n",
    "upper = input_str.upper()\n",
    "print(upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk as nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a real movie , about real people.', \"Movie gives us a rare glimpse into a culture most of us don't know .\"]\n"
     ]
    }
   ],
   "source": [
    "#sentanse tokenization\n",
    "import nltk as nl\n",
    "\n",
    "import random\n",
    "\n",
    "example_text = 'a real movie , about real people. Movie gives us a rare glimpse into a culture most of us don\\'t know .'\n",
    "print(nl.tokenize.sent_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Movie', 'gives', 'us', 'a', 'rare', 'glimpse', 'into', 'a', 'culture', 'most', 'of', 'us', 'do', \"n't\", 'know', '.']\n"
     ]
    }
   ],
   "source": [
    "#word tokenization\n",
    "example = 'Movie gives us a rare glimpse into a culture most of us don\\'t know .'\n",
    "\n",
    "tokenized_words = nl.tokenize.word_tokenize(example)\n",
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Movie', 'NNP'), ('gives', 'VBZ'), ('us', 'PRP'), ('a', 'DT'), ('rare', 'JJ'), ('glimpse', 'NN'), ('into', 'IN'), ('a', 'DT'), ('culture', 'NN'), ('most', 'JJS'), ('of', 'IN'), ('us', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('know', 'VB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#pos tagging\n",
    "tagged_words = nl.tag.pos_tag(tokenized_words)\n",
    "print(tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop words provided by NLTK package----\n",
      "\n",
      "{'then', 'with', 'do', 'itself', 'any', 'such', 'ma', 'weren', 'can', 'not', 'am', 'those', 'the', 'than', 'has', 're', 'have', 'own', 'these', 'both', 'he', 'you', 'until', 'here', 'down', 'their', 'because', 'when', 'each', 'his', 'we', 'are', 'if', 'other', 'had', 'out', 'o', 'wasn', 't', 'again', 'just', 'being', 'my', 'there', 'themselves', 'ours', 'our', 'ourselves', 'ain', 'before', 'myself', 'or', 'doing', 'nor', 'in', 'more', 've', 'haven', 'herself', 'over', 'll', 'her', 'some', 'by', 'all', 'up', 'this', 'she', 'wouldn', 'as', 'under', 'doesn', 'me', 'i', 'should', 'mustn', 'no', 'whom', 'why', 'were', 'will', 'yours', 'about', 'while', 'into', 'shan', 'same', 'and', 'them', 'a', 'during', 'where', 'it', 'few', 'd', 'they', 'aren', 'once', 'does', 'how', 'y', 'off', 'too', 'who', 'himself', 'its', 'won', 'through', 'is', 'above', 'only', 'that', 'between', 'yourself', 'having', 'from', 'your', 'mightn', 'him', 'against', 'didn', 'be', 'an', 'yourselves', 'hers', 'shouldn', 'don', 'of', 'was', 'but', 'needn', 'to', 'been', 'most', 'further', 'm', 'hadn', 'very', 'at', 'couldn', 's', 'theirs', 'for', 'on', 'now', 'below', 'which', 'isn', 'did', 'so', 'hasn', 'after', 'what'}\n",
      "\n",
      " filtered data after stop word removal \n",
      "\n",
      "['Movie', 'gives', 'us', 'rare', 'glimpse', 'culture', 'us', \"n't\", 'know', '.']\n"
     ]
    }
   ],
   "source": [
    "# removal of stop words\n",
    "\n",
    "\n",
    "stop_words = set(nl.corpus.stopwords.words('english'))\n",
    "\n",
    "word_tokens = nl.tokenize.word_tokenize(example)\n",
    "\n",
    "filtered_words = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_words.append(w)\n",
    "print('stop words provided by NLTK package----\\n')\n",
    "print(stop_words)\n",
    "print('\\n filtered data after stop word removal \\n')\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Movie/NNP)\n",
      "  gives/VBZ\n",
      "  us/PRP\n",
      "  a/DT\n",
      "  rare/JJ\n",
      "  glimpse/NN\n",
      "  into/IN\n",
      "  a/DT\n",
      "  culture/NN\n",
      "  most/JJS\n",
      "  of/IN\n",
      "  us/PRP\n",
      "  do/VBP\n",
      "  n't/RB\n",
      "  know/VB\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#Chunking\n",
    "chunked_data = nl.chunk.ne_chunk(tagged_words)\n",
    "print(chunked_data)\n",
    "chunked_data.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Movie', 'gives', 'us', 'rare', 'glimpse', 'culture', 'us', \"n't\", 'know', '.']\n",
      "['Movie', 'gives', 'us', 'rare', 'glimpse', 'culture', 'us', \"n't\", 'know']\n"
     ]
    }
   ],
   "source": [
    "#punctuation mark removal\n",
    "p = [',','?','.']\n",
    "word_tokens = filtered_words\n",
    "\n",
    "\n",
    "filtered2 = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in p:\n",
    "        filtered2.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movi\n",
      "give\n",
      "us\n",
      "rare\n",
      "glimps\n",
      "cultur\n",
      "us\n",
      "n't\n",
      "know\n",
      "-------------------\n",
      "bank\n",
      "bank\n",
      "bank\n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "\n",
    "ps = nl.stem.PorterStemmer()\n",
    "set1 =['Movie', 'gives', 'us', 'rare', 'glimpse', 'culture', 'us', \"n't\", 'know']\n",
    "\n",
    "for w in set1:\n",
    "    print(ps.stem(w))\n",
    "    \n",
    "set2 =[ \"python\",\"pythoner\",\"pythoning\",\"pythoned\"]\n",
    "set2 =[ \"banks\",\"banking\",\"bank\"]\n",
    "\n",
    "print('-------------------')\n",
    "for w in set2:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplistic , silly and tedious . \n",
      "\n",
      "it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      "exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      "[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      "a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data set loading\n",
    "positive = open('rt-polarity-pos.txt')\n",
    "\n",
    "negative = open('rt-polarity-neg.txt')\n",
    "\n",
    "i=0\n",
    "while i<5 :\n",
    "    print(negative.readline())\n",
    "    i+=1\n",
    "\n",
    "#print(positive.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review before processing->\n",
      "the story is also as unoriginal as they come , already having been recycled more times than i'd care to count . \n",
      "\n",
      "review after processing->\n",
      "[{'story': 1, 'also': 1, 'unoriginal': 1, 'come': 1, 'already': 1, 'recycled': 1, 'times': 1, \"'d\": 1, 'care': 1, 'count': 1}, 'negative']\n",
      "-------------------------\n",
      "review before processing->\n",
      "about the only thing to give the movie points for is bravado -- to take an entirely stale concept and push it through the audience's meat grinder one more time . \n",
      "\n",
      "review after processing->\n",
      "[{'thing': 1, 'give': 1, 'movie': 1, 'points': 1, 'bravado': 1, '--': 1, 'take': 1, 'entirely': 1, 'stale': 1, 'concept': 1, 'push': 1, 'audience': 1, \"'s\": 1, 'meat': 1, 'grinder': 1, 'one': 1, 'time': 1}, 'negative']\n",
      "-------------------------\n",
      "review before processing->\n",
      "not so much farcical as sour . \n",
      "\n",
      "review after processing->\n",
      "[{'much': 1, 'farcical': 1, 'sour': 1}, 'negative']\n",
      "-------------------------\n",
      "review before processing->\n",
      "unfortunately the story and the actors are served with a hack script . \n",
      "\n",
      "review after processing->\n",
      "[{'unfortunately': 1, 'story': 1, 'actors': 1, 'served': 1, 'hack': 1, 'script': 1}, 'negative']\n",
      "-------------------------\n",
      "review before processing->\n",
      "all the more disquieting for its relatively gore-free allusions to the serial murders , but it falls down in its attempts to humanize its subject . \n",
      "\n",
      "review after processing->\n",
      "[{'disquieting': 1, 'relatively': 1, 'gore-free': 1, 'allusions': 1, 'serial': 1, 'murders': 1, 'falls': 1, 'attempts': 1, 'humanize': 1, 'subject': 1}, 'negative']\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "#preprocessing\n",
    "def remove_stop_words(w_token):\n",
    "    stop_words = set(nl.corpus.stopwords.words('english'))\n",
    "    filtered_words = []\n",
    "    ps = nl.stem.PorterStemmer()\n",
    "    for tmp_word in w_token:\n",
    "        if tmp_word not in stop_words:\n",
    "            filtered_words.append(tmp_word)\n",
    "    return filtered_words\n",
    "    \n",
    "def process_sentence(s):\n",
    "    w_token = nl.tokenize.word_tokenize(s) \n",
    "    punctuations = [',','?','.',']','[','}','{','(',')','!','?',':',';','\"','\\'']\n",
    "    t2 = []\n",
    "    for w in w_token:\n",
    "        if w not in punctuations:\n",
    "            t2.append(w)\n",
    "    t3 = remove_stop_words(t2)\n",
    "    return {word: 1 for word in t3}\n",
    "    \n",
    "\n",
    "positive_data_array = []\n",
    "\n",
    "# positive review prcocessing\n",
    "for p_review in positive:\n",
    "    positive_data_array.append([process_sentence(p_review),'positive'])\n",
    "        \n",
    "\n",
    "\n",
    "negative_data_array = []\n",
    "i= 0\n",
    "\n",
    "#negative review processing\n",
    "for n_review in negative:\n",
    "        processed = [process_sentence(n_review),'negative']   \n",
    "        negative_data_array.append(processed)\n",
    "        \n",
    "        if(i<5):\n",
    "            print('review before processing->')            # demo purpose code to see the affect\n",
    "            print(n_review)\n",
    "            print('review after processing->')\n",
    "            print(processed)\n",
    "            print('-------------------------')\n",
    "            i+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#partition into training and test set\n",
    "\n",
    "\n",
    "# shuffling\n",
    "random.shuffle(positive_data_array)\n",
    "random.shuffle(negative_data_array)\n",
    "\n",
    "#partitioning\n",
    "training_set = positive_data_array[:3000]+negative_data_array[:3000]\n",
    "test_set =positive_data_array[3000:]+negative_data_array[3000:]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7537041013528022\n",
      "\n",
      "\n",
      "True Positive: 1822 \t False Positive: 638\n",
      "False Negative: 509 \t True Negative: 1688\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      "_________________________________________________\n",
      "\t\t\tActual Output\n",
      "\n",
      "_________________________________________________\n",
      "\t\t 1822 \t\t 638\n",
      "Test Output -------------------------------------\n",
      "\t\t 509 \t\t 1688\n",
      "_________________________________________________\n",
      "\n",
      "\n",
      "Precision:[TP/(TP+FP)]\t 0.740650406504065\n",
      "Recall:[TP/(TP+FN)]\t 0.7816387816387816\n",
      "Accuaracy:[(TP+TN)/(TP+TN+FP+FN)]\t 0.7537041013528022\n"
     ]
    }
   ],
   "source": [
    "#build classifier and test\n",
    "classifier = nl.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "\n",
    "print('Accuracy: ',nl.classify.util.accuracy(classifier,test_set))\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "TP = 0 \n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "for itr in range(len(test_set)):\n",
    "\n",
    "    \n",
    "    out=classifier.classify(test_set[itr][0])\n",
    " \n",
    "    if out=='positive' and test_set[itr][1]=='positive':\n",
    "        TP = TP + 1\n",
    "    if out=='negative' and test_set[itr][1]=='negative':\n",
    "        TN = TN + 1\n",
    "    if out=='positive' and test_set[itr][1]=='negative':\n",
    "        FP = FP + 1\n",
    "    if out=='negative' and test_set[itr][1]=='positive':\n",
    "        FN = FN + 1\n",
    "\n",
    "print('True Positive:',TP,'\\t False Positive:',FP)\n",
    "print('False Negative:',FN,'\\t True Negative:',TN)\n",
    "\n",
    "print('\\n')\n",
    "print('Confusion Matrix:\\n')\n",
    "print('_________________________________________________')\n",
    "print('\\t\\t\\tActual Output\\n')\n",
    "print('_________________________________________________')\n",
    "print('\\t\\t',TP,'\\t\\t',FP)\n",
    "print('Test Output -------------------------------------')\n",
    "print('\\t\\t',FN,'\\t\\t',TN)\n",
    "print('_________________________________________________')\n",
    "print('\\n')\n",
    "print('Precision:[TP/(TP+FP)]\\t',TP/(TP+FP))\n",
    "print('Recall:[TP/(TP+FN)]\\t',TP/(TP+FN))\n",
    "print('Accuaracy:[(TP+TN)/(TP+TN+FP+FN)]\\t',(TP+TN)/(TP+TN+FP+FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "# classifiy new test instance\n",
    "\n",
    "print(classifier.classify(process_sentence('very bad movie. I have wasted my money.')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "print(classifier.classify(process_sentence('One of the best movie, I have ever seen.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(classifier.classify(process_sentence('a real movie , about real people , that gives us a rare glimpse into a culture most of us don\\'t know .')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('run.n.01'), Synset('test.n.05'), Synset('footrace.n.01'), Synset('streak.n.01'), Synset('run.n.05'), Synset('run.n.06'), Synset('run.n.07'), Synset('run.n.08'), Synset('run.n.09'), Synset('run.n.10'), Synset('rivulet.n.01'), Synset('political_campaign.n.01'), Synset('run.n.13'), Synset('discharge.n.06'), Synset('run.n.15'), Synset('run.n.16'), Synset('run.v.01'), Synset('scat.v.01'), Synset('run.v.03'), Synset('operate.v.01'), Synset('run.v.05'), Synset('run.v.06'), Synset('function.v.01'), Synset('range.v.01'), Synset('campaign.v.01'), Synset('play.v.18'), Synset('run.v.11'), Synset('tend.v.01'), Synset('run.v.13'), Synset('run.v.14'), Synset('run.v.15'), Synset('run.v.16'), Synset('prevail.v.03'), Synset('run.v.18'), Synset('run.v.19'), Synset('carry.v.15'), Synset('run.v.21'), Synset('guide.v.05'), Synset('run.v.23'), Synset('run.v.24'), Synset('run.v.25'), Synset('run.v.26'), Synset('run.v.27'), Synset('run.v.28'), Synset('run.v.29'), Synset('run.v.30'), Synset('run.v.31'), Synset('run.v.32'), Synset('run.v.33'), Synset('run.v.34'), Synset('ply.v.03'), Synset('hunt.v.01'), Synset('race.v.02'), Synset('move.v.13'), Synset('melt.v.01'), Synset('ladder.v.01'), Synset('run.v.41')]\n",
      "run.n.01\n",
      "run\n",
      "a score in baseball made by a runner touching all four bases safely\n",
      "['the Yankees scored 3 runs in the bottom of the 9th', 'their first tally came in the 3rd inning']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk \n",
    "# Then, we're going to use the term \"program\" to find synsets like so:\n",
    "syns = wn.synsets(\"run\")\n",
    " \n",
    "print(syns)\n",
    "# An example of a synset:\n",
    "print(syns[0].name())\n",
    " \n",
    "# Just the word:\n",
    "print(syns[0].lemmas()[0].name())\n",
    " \n",
    "# Definition of that first synset:\n",
    "print(syns[0].definition())\n",
    " \n",
    "# Examples of the word in use in sentences:\n",
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise\n",
    "\n",
    "## e1. You two text files as follows:\n",
    "#####  apple_comp_data.txt: contains some sentences about apple company.\n",
    "#####  apple_fruit_data.txt: contains some sentences about apple fruit.\n",
    "\n",
    "### 1.1 Load the text data from the files.\n",
    "### 1.2 Preprocess the Data.\n",
    "### 1.3 Apply NB classifier.\n",
    "### 1.4 Print the Performance.\n",
    "\n",
    "## e2. You have 5 text documents in zipped format (docs.zip).\n",
    "\n",
    "### 1.2 Load text data from the files.\n",
    "### 1.2 Preprocess the data.\n",
    "### Create document-term matrix.[don't use inbuilt utility]\n",
    "\n",
    "#####  Document-Term Matrix: A document-term matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.\n",
    "\n",
    "## Example: \n",
    "##### d1-  i like physics.\n",
    "##### d2- she hates physics.\n",
    "\n",
    "|   |    i | like | physics |  she  | hate |\n",
    "|---|------|------|---------|-------|------|\n",
    "| d1|   1  |  1   | 1       |  0    | 0    |\n",
    "| d2|   0  |  0   | 1       |  1    | 1    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
